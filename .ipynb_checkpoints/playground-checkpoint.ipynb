{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "DATAFRAME_PATH = './data/labeled-comments.csv'\n",
    "TF_QUANTITY = 100\n",
    "\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "\n",
    "def get_vocabulary(df):\n",
    "    count_vectorizer = CountVectorizer(lowercase=False, stop_words=[])\n",
    "    cv_fit = count_vectorizer.fit_transform(df['content'])\n",
    "    word_list = count_vectorizer.get_feature_names()\n",
    "    frequecy_array = cv_fit.toarray()\n",
    "    count_list = frequecy_array.sum(axis=0)\n",
    "    vocabulary = (dict(zip(word_list, count_list)))\n",
    "    return vocabulary, frequecy_array, word_list\n",
    "\n",
    "\n",
    "def get_bigrams(df):\n",
    "    count_vectorizer = CountVectorizer(\n",
    "        lowercase=False, stop_words=[], ngram_range=(2, 2))\n",
    "    cv_fit = count_vectorizer.fit_transform(df['content'])\n",
    "    word_list = count_vectorizer.get_feature_names()\n",
    "    frequecy_array = cv_fit.toarray()\n",
    "    count_list = frequecy_array.sum(axis=0)\n",
    "    vocabulary = (dict(zip(word_list, count_list)))\n",
    "    return vocabulary, frequecy_array, word_list\n",
    "\n",
    "\n",
    "def get_doc(df, chosen_words):\n",
    "    return df['content'].apply(lambda y: ' '.join(\n",
    "        [x for x in y.split() if x in chosen_words]))\n",
    "\n",
    "\n",
    "def get_bigram_doc(df, chosen_words):\n",
    "    return df['content'].apply(lambda text: ' '.join(\n",
    "        [' '.join(w) for w in\n",
    "         [b for l in text for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "            if ' '.join(w) in chosen_words]\n",
    "    ))\n",
    "\n",
    "\n",
    "def get_relevant_words(df):\n",
    "    return list(df.sort_values(\n",
    "        by=['diff'], ascending=False)['word'])\n",
    "\n",
    "\n",
    "class Comments:\n",
    "    def __init__(self, conn=None):\n",
    "        try:\n",
    "            labeled_comments = pd.read_csv(DATAFRAME_PATH)\n",
    "        except:\n",
    "            print('Retrieving from original database...')\n",
    "            labeled_comments = pd.read_sql_query(\n",
    "                'select * from results;', conn)\n",
    "            labeled_comments['label'] = labeled_comments['avg'].apply(\n",
    "                lambda x: 1 if x > 0.5 else 0 if x < 0.5 else -1)\n",
    "            labeled_comments['char-qty'] = labeled_comments['content'].apply(\n",
    "                lambda comment: len(comment))\n",
    "            labeled_comments['word-qty'] = labeled_comments['content'].apply(\n",
    "                lambda comment: len(comment.lower().split(' ')))\n",
    "            labeled_comments.to_csv(DATAFRAME_PATH, index=False)\n",
    "\n",
    "        self.labeled_comments = labeled_comments\n",
    "        self.sexist_comments = labeled_comments[labeled_comments['avg'] > 0.5]\n",
    "        self.not_sexist_comments = labeled_comments[labeled_comments['avg'] < 0.5]\n",
    "        self.undefined_comments = labeled_comments[labeled_comments['avg'] == 0.5]\n",
    "\n",
    "        self._set_vocabularies()\n",
    "        self._set_word_frequencies()\n",
    "        self._set_bigrams_frequencies()\n",
    "        self._set_feature_dataframe()\n",
    "\n",
    "    def _set_vocabularies(self):\n",
    "        self.vocabulary, self.freq, self.word_list = get_vocabulary(\n",
    "            self.labeled_comments)\n",
    "        self.sexist_vocabulary, self.sexist_frequency_array, self.sexist_word_list = get_vocabulary(\n",
    "            self.sexist_comments)\n",
    "        self.not_sexist_vocabulary, self.not_sexist_frequency_array, self.not_sexist_word_list = get_vocabulary(\n",
    "            self.not_sexist_comments)\n",
    "        self.undefined_vocabulary, self.undefined_frequency_array, self.undefined_word_list = get_vocabulary(\n",
    "            self.undefined_comments)\n",
    "\n",
    "        self.bigrams, self.freq, self.word_list = get_bigrams(\n",
    "            self.labeled_comments)\n",
    "        self.sexist_bigrams, self.sexist_frequency_array, self.sexist_word_list = get_bigrams(\n",
    "            self.sexist_comments)\n",
    "        self.not_sexist_bigrams, self.not_sexist_frequency_array, self.not_sexist_word_list = get_bigrams(\n",
    "            self.not_sexist_comments)\n",
    "        self.undefined_bigrams, self.undefined_frequency_array, self.undefined_word_list = get_bigrams(\n",
    "            self.undefined_comments)\n",
    "\n",
    "    def _set_bigrams_frequencies(self):\n",
    "        word_freq = {\n",
    "            'word': [],\n",
    "            'sexist-freq': [],\n",
    "            'not-sexist-freq': [],\n",
    "            'undefined-freq': [],\n",
    "            'diff': []\n",
    "        }\n",
    "\n",
    "        # select words that are in bolth vocabularies\n",
    "        list_sexist_sorted_terms = []\n",
    "        for key, value in sorted(self.sexist_bigrams.items(), key=lambda item: item[1]):\n",
    "            list_sexist_sorted_terms.append(key)\n",
    "\n",
    "        shared_words = []\n",
    "\n",
    "        for word in list_sexist_sorted_terms:\n",
    "            if word in self.not_sexist_bigrams.keys():\n",
    "                shared_words.append(word)\n",
    "\n",
    "        for word in shared_words:\n",
    "            word_freq['word'].append(word)\n",
    "            if word in self.sexist_bigrams.keys():\n",
    "                word_freq['sexist-freq'].append(self.sexist_bigrams[word])\n",
    "            else:\n",
    "                word_freq['sexist-freq'].append(0)\n",
    "            if word in self.sexist_bigrams.keys():\n",
    "                word_freq['not-sexist-freq'].append(\n",
    "                    self.not_sexist_bigrams[word])\n",
    "            else:\n",
    "                word_freq['not-sexist-freq'].append(0)\n",
    "            if word in self.undefined_bigrams.keys():\n",
    "                word_freq['undefined-freq'].append(\n",
    "                    self.undefined_bigrams[word])\n",
    "            else:\n",
    "                word_freq['undefined-freq'].append(0)\n",
    "            word_freq['diff'] = self.sexist_bigrams[word] - \\\n",
    "                self.not_sexist_bigrams[word]\n",
    "\n",
    "        word_freq = pd.DataFrame(word_freq)\n",
    "\n",
    "        # normalizind frequencies\n",
    "        sum_sexist = sum(word_freq['sexist-freq'])\n",
    "        word_freq['sexist-freq'] = word_freq['sexist-freq'].apply(\n",
    "            lambda x: x/sum_sexist)\n",
    "        sum_not_sexist = sum(word_freq['not-sexist-freq'])\n",
    "        word_freq['not-sexist-freq'] = word_freq['not-sexist-freq'].apply(\n",
    "            lambda x: x/sum_not_sexist)\n",
    "        undefined_sexist = sum(word_freq['undefined-freq'])\n",
    "        word_freq['undefined-freq'] = word_freq['undefined-freq'].apply(\n",
    "            lambda x: x/undefined_sexist)\n",
    "        word_freq['diff'] = word_freq['sexist-freq'] - \\\n",
    "            word_freq['not-sexist-freq']\n",
    "\n",
    "        sexist_words = word_freq[word_freq['diff'] > 0]\n",
    "        not_sexist_words = word_freq[word_freq['diff'] < 0]\n",
    "\n",
    "        # most relevant words to sexist comments\n",
    "        self.sexist_bigrams = sexist_words.sort_values(\n",
    "            by='diff', ascending=False)\n",
    "\n",
    "        # most relevant words to not sexist comments\n",
    "        self.not_sexist_bigrams = not_sexist_words.sort_values(\n",
    "            by='diff', ascending=True)\n",
    "\n",
    "    def _set_word_frequencies(self):\n",
    "        word_freq = {\n",
    "            'word': [],\n",
    "            'sexist-freq': [],\n",
    "            'not-sexist-freq': [],\n",
    "            'undefined-freq': [],\n",
    "            'diff': []\n",
    "        }\n",
    "\n",
    "        # select words that are in bolth vocabularies\n",
    "        list_sexist_sorted_terms = []\n",
    "        for key, value in sorted(self.sexist_vocabulary.items(), key=lambda item: item[1]):\n",
    "            list_sexist_sorted_terms.append(key)\n",
    "\n",
    "        shared_words = []\n",
    "\n",
    "        for word in list_sexist_sorted_terms:\n",
    "            if word in self.not_sexist_vocabulary.keys():\n",
    "                shared_words.append(word)\n",
    "\n",
    "        for word in shared_words:\n",
    "            word_freq['word'].append(word)\n",
    "            if word in self.sexist_vocabulary.keys():\n",
    "                word_freq['sexist-freq'].append(self.sexist_vocabulary[word])\n",
    "            else:\n",
    "                word_freq['sexist-freq'].append(0)\n",
    "            if word in self.sexist_vocabulary.keys():\n",
    "                word_freq['not-sexist-freq'].append(\n",
    "                    self.not_sexist_vocabulary[word])\n",
    "            else:\n",
    "                word_freq['not-sexist-freq'].append(0)\n",
    "            if word in self.undefined_vocabulary.keys():\n",
    "                word_freq['undefined-freq'].append(\n",
    "                    self.undefined_vocabulary[word])\n",
    "            else:\n",
    "                word_freq['undefined-freq'].append(0)\n",
    "            word_freq['diff'] = self.sexist_vocabulary[word] - \\\n",
    "                self.not_sexist_vocabulary[word]\n",
    "\n",
    "        word_freq = pd.DataFrame(word_freq)\n",
    "\n",
    "        # normalizind frequencies\n",
    "        sum_sexist = sum(word_freq['sexist-freq'])\n",
    "        word_freq['sexist-freq'] = word_freq['sexist-freq'].apply(\n",
    "            lambda x: x/sum_sexist)\n",
    "        sum_not_sexist = sum(word_freq['not-sexist-freq'])\n",
    "        word_freq['not-sexist-freq'] = word_freq['not-sexist-freq'].apply(\n",
    "            lambda x: x/sum_not_sexist)\n",
    "        undefined_sexist = sum(word_freq['undefined-freq'])\n",
    "        word_freq['undefined-freq'] = word_freq['undefined-freq'].apply(\n",
    "            lambda x: x/undefined_sexist)\n",
    "        word_freq['diff'] = word_freq['sexist-freq'] - \\\n",
    "            word_freq['not-sexist-freq']\n",
    "\n",
    "        sexist_words = word_freq[word_freq['diff'] > 0]\n",
    "        not_sexist_words = word_freq[word_freq['diff'] < 0]\n",
    "\n",
    "        # most relevant words to sexist comments\n",
    "        self.sexist_words = sexist_words.sort_values(\n",
    "            by='diff', ascending=False)\n",
    "\n",
    "        # most relevant words to not sexist comments\n",
    "        self.not_sexist_words = not_sexist_words.sort_values(\n",
    "            by='diff', ascending=True)\n",
    "\n",
    "    def set_tf(self):\n",
    "        sexist_vectorizer = TfidfVectorizer(\n",
    "            stop_words=[],\n",
    "            use_idf=False,\n",
    "            norm=None,\n",
    "            decode_error='replace',\n",
    "            max_features=100,\n",
    "        )\n",
    "        not_sexist_vectorizer = TfidfVectorizer(\n",
    "            stop_words=[],\n",
    "            use_idf=False,\n",
    "            decode_error='replace',\n",
    "            max_features=100,\n",
    "        )\n",
    "        sexist_bigrams_vectorizer = TfidfVectorizer(\n",
    "            stop_words=[],\n",
    "            use_idf=False,\n",
    "            ngram_range=(2, 2),\n",
    "            decode_error='replace',\n",
    "            max_features=100,\n",
    "        )\n",
    "        not_sexist_bigrams_vectorizer = TfidfVectorizer(\n",
    "            stop_words=[],\n",
    "            use_idf=False,\n",
    "            ngram_range=(2, 2),\n",
    "            decode_error='replace',\n",
    "            max_features=100,\n",
    "        )\n",
    "\n",
    "        relevant_sexist_words = get_relevant_words(self.sexist_words)\n",
    "        relevant_not_sexist_words = get_relevant_words(\n",
    "            self.not_sexist_words)\n",
    "\n",
    "        relevant_sexist_bigrams = get_relevant_words(self.sexist_bigrams)\n",
    "        relevant_not_sexist_bigrams = get_relevant_words(\n",
    "            self.not_sexist_bigrams)\n",
    "\n",
    "        # tf sexist words in doc\n",
    "        sexist_doc = get_doc(self.sexist_comments,\n",
    "                             relevant_sexist_words)\n",
    "        not_sexist_doc = get_doc(\n",
    "            self.not_sexist_comments, relevant_sexist_words)\n",
    "        sexist_tf = pd.DataFrame(\n",
    "            sexist_vectorizer.fit_transform(sexist_doc).toarray())\n",
    "        not_sexist_tf = pd.DataFrame(\n",
    "            not_sexist_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
    "        self.tf_sexist_dataframe = pd.concat(\n",
    "            [sexist_tf, not_sexist_tf]).fillna(0)\n",
    "\n",
    "        sexist_doc = get_doc(self.sexist_comments,\n",
    "                             relevant_sexist_bigrams)\n",
    "        not_sexist_doc = get_bigram_doc(\n",
    "            self.not_sexist_comments, relevant_not_sexist_bigrams)\n",
    "        sexist_bigrams_tf = pd.DataFrame(\n",
    "            sexist_bigrams_vectorizer.fit_transform(sexist_doc).toarray())\n",
    "        not_sexist_bigrams_tf = pd.DataFrame(\n",
    "            not_sexist_bigrams_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
    "        self.tf_sexist_bigrams_dataframe = pd.concat(\n",
    "            [sexist_bigrams_tf, not_sexist_bigrams_tf]).fillna(0)\n",
    "\n",
    "        # tf not sexist words in doc\n",
    "        sexist_doc = get_doc(self.sexist_comments,\n",
    "                             relevant_not_sexist_words)\n",
    "        not_sexist_doc = get_doc(\n",
    "            self.not_sexist_comments, relevant_not_sexist_words)\n",
    "        sexist_tf = pd.DataFrame(\n",
    "            sexist_vectorizer.fit_transform(sexist_doc).toarray())\n",
    "        not_sexist_tf = pd.DataFrame(\n",
    "            not_sexist_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
    "        self.tf_not_sexist_dataframe = pd.concat(\n",
    "            [sexist_tf, not_sexist_tf]).fillna(0)\n",
    "\n",
    "        sexist_doc = get_bigram_doc(self.sexist_comments,\n",
    "                                    relevant_sexist_bigrams)\n",
    "        not_sexist_doc = get_doc(\n",
    "            self.not_sexist_comments, relevant_not_sexist_bigrams)\n",
    "        sexist_bigrams_tf = pd.DataFrame(\n",
    "            sexist_bigrams_vectorizer.fit_transform(sexist_doc).toarray())\n",
    "        not_sexist_bigrams_tf = pd.DataFrame(\n",
    "            not_sexist_bigrams_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
    "        self.tf_not_sexist_bigrams_dataframe = pd.concat(\n",
    "            [sexist_bigrams_tf, not_sexist_bigrams_tf]).fillna(0)\n",
    "\n",
    "    def _set_feature_dataframe(self):\n",
    "        try:\n",
    "            print('...')\n",
    "            self.dataframe = pd.read_csv('./data/dataframe.csv')\n",
    "        except:\n",
    "            print('---')\n",
    "\n",
    "            likes_df = np.array(\n",
    "                pd.concat([self.sexist_comments['likes'], self.not_sexist_comments['likes']]).fillna(0))\n",
    "            dislikes_df = np.array(pd.concat(\n",
    "                [self.sexist_comments['dislikes'], self.not_sexist_comments['dislikes']]).fillna(0))\n",
    "            char_qty_df = np.array(pd.concat(\n",
    "                [self.sexist_comments['char-qty'], self.not_sexist_comments['char-qty']]).fillna(0))\n",
    "            word_qty_df = np.array(pd.concat(\n",
    "                [self.sexist_comments['word-qty'], self.not_sexist_comments['word-qty']]).fillna(0))\n",
    "            sexist_y = self.sexist_comments['avg'].apply(lambda x: 1)\n",
    "            not_sexist_y = self.not_sexist_comments['avg'].apply(lambda x: 0)\n",
    "            y_df = np.array(pd.concat([sexist_y, not_sexist_y]))\n",
    "\n",
    "            tf_dataframe = pd.concat(\n",
    "                [self.tf_sexist_dataframe,\n",
    "                 self.tf_not_sexist_dataframe,\n",
    "                 self.tf_sexist_bigrams_dataframe,\n",
    "                 self.tf_not_sexist_bigrams_dataframe], axis=1)\n",
    "            dataframe = tf_dataframe\n",
    "            dataframe['likes'] = likes_df\n",
    "            dataframe['dislikes'] = dislikes_df\n",
    "            dataframe['char-qty'] = char_qty_df\n",
    "            dataframe['word-qty'] = word_qty_df\n",
    "            dataframe['sexist'] = y_df\n",
    "            dataframe = dataframe.fillna(0)\n",
    "            dataframe = tf_dataframe.sample(frac=1)\n",
    "            dataframe.to_csv('./data/dataframe.csv', index=False)\n",
    "            self.dataframe = dataframe\n",
    "\n",
    "    def print_frequecies_to_latex(self, type, limit=None):\n",
    "        swicth = {\n",
    "            'sexist-words': self.sexist_words,\n",
    "            'sw': self.sexist_words,\n",
    "            'not-sexist-words': self.not_sexist_words,\n",
    "            'nsw': self.not_sexist_words,\n",
    "            'sexist-bigrams': self.sexist_bigrams,\n",
    "            'sb': self.sexist_bigrams,\n",
    "            'not-sexist-bigrams': self.not_sexist_bigrams,\n",
    "            'nsb': self.not_sexist_bigrams,\n",
    "        }\n",
    "        chosen = swicth[type]\n",
    "        if limit is not None:\n",
    "            chosen = chosen.head(limit)\n",
    "        for i, r in chosen.iterrows():\n",
    "            print(\"{%s} & {%0.6f} & {%0.6f} & {%0.6f}\\\\\\\\\" %\n",
    "                  (r['word'], r['sexist-freq'], r['not-sexist-freq'], r['diff']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "comments = Comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8927432435e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-11eafcb96259>\u001b[0m in \u001b[0;36mset_tf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m             self.not_sexist_comments, relevant_not_sexist_bigrams)\n\u001b[1;32m    281\u001b[0m         sexist_bigrams_tf = pd.DataFrame(\n\u001b[0;32m--> 282\u001b[0;31m             sexist_bigrams_vectorizer.fit_transform(sexist_doc).toarray())\n\u001b[0m\u001b[1;32m    283\u001b[0m         not_sexist_bigrams_tf = pd.DataFrame(\n\u001b[1;32m    284\u001b[0m             not_sexist_bigrams_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \"\"\"\n\u001b[1;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1204\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1132\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m   1135\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "comments.set_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "self=comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexist_vectorizer = TfidfVectorizer(\n",
    "    stop_words=[],\n",
    "    use_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=100,\n",
    ")\n",
    "not_sexist_vectorizer = TfidfVectorizer(\n",
    "    stop_words=[],\n",
    "    use_idf=False,\n",
    "    decode_error='replace',\n",
    "    max_features=100,\n",
    ")\n",
    "sexist_bigrams_vectorizer = TfidfVectorizer(\n",
    "    stop_words=[],\n",
    "    use_idf=False,\n",
    "    ngram_range=(2, 2),\n",
    "    decode_error='replace',\n",
    "    max_features=100,\n",
    ")\n",
    "not_sexist_bigrams_vectorizer = TfidfVectorizer(\n",
    "    stop_words=[],\n",
    "    use_idf=False,\n",
    "    ngram_range=(2, 2),\n",
    "    decode_error='replace',\n",
    "    max_features=100,\n",
    ")\n",
    "\n",
    "relevant_sexist_words = get_relevant_words(self.sexist_words)\n",
    "relevant_not_sexist_words = get_relevant_words(\n",
    "    self.not_sexist_words)\n",
    "\n",
    "relevant_sexist_bigrams = get_relevant_words(self.sexist_bigrams)\n",
    "relevant_not_sexist_bigrams = get_relevant_words(\n",
    "    self.not_sexist_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexist_doc = get_doc(self.sexist_comments,\n",
    "                             relevant_sexist_words)\n",
    "not_sexist_doc = get_doc(\n",
    "    self.not_sexist_comments, relevant_sexist_words)\n",
    "sexist_tf = pd.DataFrame(\n",
    "    sexist_vectorizer.fit_transform(sexist_doc).toarray())\n",
    "not_sexist_tf = pd.DataFrame(\n",
    "    not_sexist_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
    "self.tf_sexist_dataframe = pd.concat(\n",
    "    [sexist_tf, not_sexist_tf]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(df, chosen_words):\n",
    "    return df['content'].apply(lambda y: ' '.join(\n",
    "        [x for x in y.split() if x in chosen_words]))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# def get_bigram_doc(df, chosen_words):\n",
    "#     return df['content'].apply(lambda text: ' '.join(\n",
    "#         [' '.join(w) for w in\n",
    "#          [b for l in text for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "#             if ' '.join(w) in chosen_words]\n",
    "#     ))\n",
    "\n",
    "def get_bigram_doc(df, chosen_words):\n",
    "    def select_only_relevant_bigrams(text):\n",
    "        bigrams_in_text = [b for l in text for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "        return bigrams_in_text\n",
    "    \n",
    "    return df['content'].apply(select_only_relevant_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4        \n",
       "6        \n",
       "7        \n",
       "8        \n",
       "10       \n",
       "       ..\n",
       "3158     \n",
       "3160     \n",
       "3164     \n",
       "3168     \n",
       "3170     \n",
       "Name: content, Length: 1397, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bigram_doc(self.sexist_comments,relevant_sexist_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8e4cbc42c7e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     self.not_sexist_comments, relevant_not_sexist_bigrams)\n\u001b[1;32m      5\u001b[0m sexist_bigrams_tf = pd.DataFrame(\n\u001b[0;32m----> 6\u001b[0;31m     sexist_bigrams_vectorizer.fit_transform(sexist_doc).toarray())\n\u001b[0m\u001b[1;32m      7\u001b[0m not_sexist_bigrams_tf = pd.DataFrame(\n\u001b[1;32m      8\u001b[0m     not_sexist_bigrams_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \"\"\"\n\u001b[1;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1204\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1132\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m   1135\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "sexist_doc = get_bigram_doc(self.sexist_comments,\n",
    "                     relevant_sexist_bigrams)\n",
    "not_sexist_doc = get_bigram_doc(\n",
    "    self.not_sexist_comments, relevant_not_sexist_bigrams)\n",
    "sexist_bigrams_tf = pd.DataFrame(\n",
    "    sexist_bigrams_vectorizer.fit_transform(sexist_doc).toarray())\n",
    "not_sexist_bigrams_tf = pd.DataFrame(\n",
    "    not_sexist_bigrams_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
    "self.tf_sexist_bigrams_dataframe = pd.concat(\n",
    "    [sexist_bigrams_tf, not_sexist_bigrams_tf]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4        \n",
       "6        \n",
       "7        \n",
       "8        \n",
       "10       \n",
       "       ..\n",
       "3158     \n",
       "3160     \n",
       "3164     \n",
       "3168     \n",
       "3170     \n",
       "Name: content, Length: 1397, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bigram_doc(self.sexist_comments,\n",
    "                     relevant_sexist_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4                          Eu tô nem aí mulher de família\n",
       "6       tem pra pra meninas do ficar casa mais as meni...\n",
       "7                                                      de\n",
       "8                                                        \n",
       "10                                                       \n",
       "                              ...                        \n",
       "3158                            outra grande diz coisa Há\n",
       "3160       As mulheres querem fazer os Pois papel casa do\n",
       "3164    Homens do dos de eram se alguma São motivo de ...\n",
       "3168                                                     \n",
       "3170                                                     \n",
       "Name: content, Length: 1397, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_doc(self.sexist_comments,relevant_sexist_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf not sexist words in doc\n",
    "sexist_doc = get_doc(self.sexist_comments,\n",
    "                     relevant_not_sexist_words)\n",
    "not_sexist_doc = get_doc(\n",
    "    self.not_sexist_comments, relevant_not_sexist_words)\n",
    "sexist_tf = pd.DataFrame(\n",
    "    sexist_vectorizer.fit_transform(sexist_doc).toarray())\n",
    "not_sexist_tf = pd.DataFrame(\n",
    "    not_sexist_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
    "self.tf_not_sexist_dataframe = pd.concat(\n",
    "    [sexist_tf, not_sexist_tf]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4        \n",
       "6        \n",
       "7        \n",
       "8        \n",
       "10       \n",
       "       ..\n",
       "3158     \n",
       "3160     \n",
       "3164     \n",
       "3168     \n",
       "3170     \n",
       "Name: content, Length: 1397, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bigram_doc(self.sexist_comments,\n",
    "                            relevant_sexist_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8df5691091c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     self.not_sexist_comments, relevant_not_sexist_bigrams)\n\u001b[1;32m      5\u001b[0m sexist_bigrams_tf = pd.DataFrame(\n\u001b[0;32m----> 6\u001b[0;31m     sexist_bigrams_vectorizer.fit_transform(sexist_doc).toarray())\n\u001b[0m\u001b[1;32m      7\u001b[0m not_sexist_bigrams_tf = pd.DataFrame(\n\u001b[1;32m      8\u001b[0m     not_sexist_bigrams_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \"\"\"\n\u001b[1;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1204\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1132\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m   1135\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "sexist_doc = get_bigram_doc(self.sexist_comments,\n",
    "                            relevant_sexist_bigrams)\n",
    "not_sexist_doc = get_bigram_doc(\n",
    "    self.not_sexist_comments, relevant_not_sexist_bigrams)\n",
    "sexist_bigrams_tf = pd.DataFrame(\n",
    "    sexist_bigrams_vectorizer.fit_transform(sexist_doc).toarray())\n",
    "not_sexist_bigrams_tf = pd.DataFrame(\n",
    "    not_sexist_bigrams_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
    "self.tf_not_sexist_bigrams_dataframe = pd.concat(\n",
    "    [sexist_bigrams_tf, not_sexist_bigrams_tf]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
