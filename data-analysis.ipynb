{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stringcase in /Users/luisabraga/Library/Python/3.9/lib/python/site-packages (1.2.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (3.0.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (9.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (1.22.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (4.28.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.9/site-packages (3.6.7)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.9/site-packages (from nltk) (2021.11.10)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: wordcloud in /opt/homebrew/lib/python3.9/site-packages (1.8.1)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.9/site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/homebrew/lib/python3.9/site-packages (from wordcloud) (1.22.1)\n",
      "Requirement already satisfied: pillow in /opt/homebrew/lib/python3.9/site-packages (from wordcloud) (9.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib->wordcloud) (4.28.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib->wordcloud) (3.0.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install stringcase --user\n",
    "! pip3 install matplotlib\n",
    "! pip3 install nltk\n",
    "! pip3 install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import json\n",
    "\n",
    "import uuid\n",
    "import datetime\n",
    "import re\n",
    "import stringcase\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database: [ec2-54-84-35-171.compute-1.amazonaws.com] @ 0 seconds\n"
     ]
    }
   ],
   "source": [
    "from modules.database_connector import Database\n",
    "\n",
    "database = Database()\n",
    "conn = database.get_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_colors = ['#FF7CB9','#979FEF']\n",
    "csfont = {'fontname':'Helvetica'}\n",
    "\n",
    "remove_special_char = lambda word: re.sub(r'[!.,\\-\";:~><^%$#@&*\\(\\)-+=_\\'\\?]','', word)\n",
    "\n",
    "def calculate_age(birth): \n",
    "    age = today.year - birth.year - ((today.month, today.day) < (birth.month, birth.day)) \n",
    "    return age\n",
    "\n",
    "def define_class(x):\n",
    "    if x > 0.5:\n",
    "        return 1\n",
    "    elif x < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "def get_vocabulary(df):\n",
    "    count_vectorizer = CountVectorizer(lowercase=False,stop_words=[])\n",
    "    cv_fit = count_vectorizer.fit_transform(df['content'])\n",
    "    word_list = count_vectorizer.get_feature_names()\n",
    "    frequecy_array = cv_fit.toarray()\n",
    "    count_list = frequecy_array.sum(axis=0)\n",
    "    vocabulary = (dict(zip(word_list, count_list)))\n",
    "    return vocabulary, frequecy_array, word_list\n",
    "\n",
    "def normalize_dict(df):\n",
    "    aux_df = dict({})\n",
    "    for key in df.keys():\n",
    "        aux_df[key] = np.round(df[key]/sum(df.values()),2)*100\n",
    "    return aux_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_info = dict({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving from original database...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "#     raise Exception\n",
    "    users_df = pd.read_csv('data/users-df.csv')\n",
    "except:\n",
    "    print('Retrieving from original database...')\n",
    "    users_df = pd.read_sql_query('select username, email, birth, gender from users;',conn)\n",
    "    users_df['age'] = users_df['birth'].apply(calculate_age)\n",
    "    users_df.to_csv('data/users-df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_users = users_df.groupby(['gender'])['username'].count()\n",
    "db_info['users'] = {}\n",
    "db_info['users']['total'] = count_users['fem'] + count_users['masc'] + count_users['other']\n",
    "db_info['users']['fem'] = count_users['fem']\n",
    "db_info['users']['mas'] = count_users['masc']\n",
    "db_info['users']['other'] = count_users['other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_users = dict({})\n",
    "\n",
    "temp_users['total'] = count_votes_by_gender['fem']+count_votes_by_gender['masc']\n",
    "temp_users['female'] = count_votes_by_gender['fem']\n",
    "temp_users['male'] = count_votes_by_gender['masc']\n",
    "db_info['users'] = temp_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'users': {'total': 573, 'fem': 340, 'mas': 231, 'other': 2}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.classification.read_dataframe import Dataframe\n",
    "\n",
    "data = Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving from original database...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pk/3cblwdc92rn9h3t4kf03y_940000gp/T/ipykernel_38041/511286968.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     raise Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlabeled_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/labeled-comments.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pk/3cblwdc92rn9h3t4kf03y_940000gp/T/ipykernel_38041/511286968.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Retrieving from original database...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlabeled_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'select * from results;'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mlabeled_comments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabeled_comments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'avg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlabeled_comments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'char-qty'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabeled_comments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "#     raise Exception\n",
    "    labeled_comments = pd.read_csv('./data/labeled-comments.csv')\n",
    "except:\n",
    "    print('Retrieving from original database...')\n",
    "    labeled_comments = pd.read_sql_query('select * from results;',conn)\n",
    "    labeled_comments['label'] = labeled_comments['avg'].apply(lambda x: 1 if x > 0.5 else 0 if x < 0.5 else -1)\n",
    "    labeled_comments['char-qty'] = labeled_comments['content'].apply(lambda comment: len(comment))\n",
    "    labeled_comments['word-qty'] = labeled_comments['content'].apply(lambda comment: len(comment.lower().split(' ')))\n",
    "    labeled_comments.to_csv('./data/labeled-comments.csv', index=False)\n",
    "\n",
    "sexist_comments = labeled_comments[labeled_comments['avg'] > 0.5]\n",
    "not_sexist_comments = labeled_comments[labeled_comments['avg'] < 0.5]\n",
    "undefined_comments = labeled_comments[labeled_comments['avg'] == 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocabulary, freq, word_list = get_vocabulary(labeled_comments)\n",
    "\n",
    "sexist_vocabulary, sexist_frequency_array, sexist_word_list = get_vocabulary(sexist_comments)\n",
    "not_sexist_vocabulary, not_sexist_frequency_array, not_sexist_word_list = get_vocabulary(not_sexist_comments)\n",
    "undefined_vocabulary, undefined_frequency_array, undefined_word_list = get_vocabulary(undefined_comments)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "print('Total terms:',len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "stopwords_in_comments = [x for x in vocabulary.keys() if x in pt_stopwords]\n",
    "print('Total stopwords:',len(stopwords_in_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_in_voc = []\n",
    "for x in vocabulary.keys():\n",
    "    for char in x:\n",
    "        if char in '0123456789' and x not in number_in_voc:\n",
    "            try:\n",
    "                int(x)\n",
    "            except:\n",
    "                number_in_voc.append(x)\n",
    "\n",
    "print('Total alphanumeric terms:', len(number_in_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in vocabulary.keys():\n",
    "    if term in number_in_voc:\n",
    "        print(term, vocabulary[term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud -q\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = ['MULHER', 'MULHERES', 'FEMINISMO', 'FEMINISTA', 'ASSÉDIO', 'FEMINISTAS', 'mulheres', 'mulher', 'assédio']\n",
    "all_summary = \" \".join(s.upper() for s in sexist_comments['content'])\n",
    "all_summary = ' '.join([a for a in all_summary.split(' ') if a not in remove_words ])\n",
    "wordcloud = WordCloud(stopwords=pt_stopwords + ['pra', 'desse', 'vc', 'nesse', 'algum', 'dessa', 'aqui', 'ser', 'cada'],\n",
    "                      background_color='white', width=1600,\n",
    "                      max_words=100,\n",
    "                      height=800).generate(all_summary.lower())\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.set_axis_off()\n",
    "plt.imshow(wordcloud)\n",
    "wordcloud.to_file('rafael.png',);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_comments = dict({})\n",
    "count_comments = labeled_comments.groupby(['label'])['comment_id'].count()\n",
    "temp_comments['total'] = labeled_comments.shape[0]\n",
    "temp_comments['sexist'] = {\n",
    "    'total_qty': count_comments[1],\n",
    "    'likes_qty': sexist_comments.sum()['likes'],\n",
    "    'dislikes_qty': sexist_comments.sum()['dislikes'],\n",
    "    'likes_qty_median': sexist_comments.median()['likes'],\n",
    "    'dislikes_qty_median': sexist_comments.median()['dislikes'],\n",
    "    'char_qty_median': sexist_comments.median()['char-qty'],\n",
    "    'word_qty_median': sexist_comments.median()['word-qty']\n",
    "}\n",
    "temp_comments['not_sexist'] = {\n",
    "    'total_qty': count_comments[0],\n",
    "    'likes_qty': not_sexist_comments.sum()['likes'],\n",
    "    'dislikes_qty': not_sexist_comments.sum()['dislikes'],\n",
    "    'likes_qty_median': not_sexist_comments.median()['likes'],\n",
    "    'dislikes_qty_median': not_sexist_comments.median()['dislikes'],\n",
    "    'char_qty_median': not_sexist_comments.median()['char-qty'],\n",
    "    'word_qty_median': not_sexist_comments.median()['word-qty']\n",
    "}\n",
    "temp_comments['undefined'] = {\n",
    "    'total_qty': count_comments[-1],\n",
    "    'likes_qty': undefined_comments.sum()['likes'],\n",
    "    'dislikes_qty': undefined_comments.sum()['dislikes'],\n",
    "    'likes_qty_median': undefined_comments.median()['likes'],\n",
    "    'dislikes_qty_median': undefined_comments.median()['dislikes'],\n",
    "    'char_qty_median': undefined_comments.median()['char-qty'],\n",
    "    'word_qty_median': undefined_comments.median()['word-qty']\n",
    "}\n",
    "db_info['comments'] = temp_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shared words between sexist and not sexist comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sexist_sorted_terms = []\n",
    "for key, value in sorted(sexist_vocabulary.items(), key=lambda item: item[1]):\n",
    "    list_sexist_sorted_terms.append(key)\n",
    "list_sexist_sorted_terms.reverse()\n",
    "\n",
    "shared_words = []\n",
    "\n",
    "for word in list_sexist_sorted_terms:\n",
    "    if word in not_sexist_vocabulary.keys():\n",
    "        shared_words.append(word)\n",
    "\n",
    "word_freq = {\n",
    "    'word': [],\n",
    "    'sexist-freq': [],\n",
    "    'not-sexist-freq': [],\n",
    "    'undefined-freq' : [],\n",
    "    'diff' : []\n",
    "}\n",
    "\n",
    "for word in shared_words:\n",
    "    word_freq['word'].append(word)\n",
    "    if word in sexist_vocabulary.keys():\n",
    "      word_freq['sexist-freq'].append(sexist_vocabulary[word])\n",
    "    else:\n",
    "      word_freq['sexist-freq'].append(0)\n",
    "    if word in sexist_vocabulary.keys():\n",
    "      word_freq['not-sexist-freq'].append(not_sexist_vocabulary[word])\n",
    "    else:\n",
    "      word_freq['not-sexist-freq'].append(0)\n",
    "    if word in undefined_vocabulary.keys():\n",
    "        word_freq['undefined-freq'].append(undefined_vocabulary[word])\n",
    "    else:\n",
    "        word_freq['undefined-freq'].append(0)\n",
    "    word_freq['diff'] = sexist_vocabulary[word] - not_sexist_vocabulary[word]\n",
    "\n",
    "word_freq = pd.DataFrame(word_freq)\n",
    "\n",
    "# normalizind frequencies\n",
    "sum_sexist = sum(word_freq['sexist-freq'])\n",
    "word_freq['sexist-freq'] = word_freq['sexist-freq'].apply(lambda x: x/sum_sexist)\n",
    "sum_not_sexist = sum(word_freq['not-sexist-freq'])\n",
    "word_freq['not-sexist-freq'] = word_freq['not-sexist-freq'].apply(lambda x: x/sum_not_sexist)\n",
    "undefined_sexist = sum(word_freq['undefined-freq'])\n",
    "word_freq['undefined-freq'] = word_freq['undefined-freq'].apply(lambda x: x/undefined_sexist)\n",
    "word_freq['diff'] = word_freq['sexist-freq'] - word_freq['not-sexist-freq']\n",
    "\n",
    "sexist_words = word_freq[word_freq['diff'] > 0]\n",
    "not_sexist_words = word_freq[word_freq['diff'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_sexist_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_sexist_words = list(sexist_words.sort_values(by=['diff'], ascending=False)['word'])\n",
    "\n",
    "# 20 most relevant words to sexist comments\n",
    "top20_sexist_words = sexist_words.sort_values(by ='diff',ascending=False ).head(20)\n",
    "top20_sexist_words['perc_S'] = top20_sexist_words['sexist-freq'].apply(lambda x : x)\n",
    "top20_sexist_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print table to latex\n",
    "for i, r in top20_sexist_words.iterrows():\n",
    "    print(\"{%s} & {%0.6f} & {%0.6f} & {%0.6f}\\\\\\\\\" %\n",
    "          (r['word'], r['sexist-freq'], r['not-sexist-freq'], r['diff']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiss = word_freq[word_freq['diff']>0].sort_values(by=['diff'], ascending=False)\n",
    "hisn = word_freq[word_freq['diff']<0].sort_values(by=['diff'], ascending=True)\n",
    "HS = list(hiss['sexist-freq'])\n",
    "HN = list(hisn['not-sexist-freq'])\n",
    "\n",
    "remove_special_char = lambda word: re.sub(r'[!.,\\-\";:~><^%$#@&*\\(\\)-+=_\\'\\?]','', word)\n",
    "words = list(word_freq['word'])\n",
    "\n",
    "duno_comments = [c for c in list(undefined_comments['content'])]\n",
    "duno_matriz = []\n",
    "words = list(hiss['word'])\n",
    "for comment in duno_comments:\n",
    "    hist = []\n",
    "    for word in words:\n",
    "        f = comment.split(' ').count(word)\n",
    "        hist.append(f)\n",
    "#     hist = [x/sum(hist) if sum(hist) > 0 else 0 for x in hist]\n",
    "    duno_matriz.append(hist)\n",
    "from scipy.spatial import distance\n",
    "JSD_S = []\n",
    "for line in duno_matriz:\n",
    "    dist = distance.jensenshannon(HS, line)\n",
    "    JSD_S.append(dist)\n",
    "\n",
    "duno_matriz = []\n",
    "words = list(hisn['word'])\n",
    "for comment in duno_comments:\n",
    "    hist = []\n",
    "    for word in words:\n",
    "        f = comment.split(' ').count(word)\n",
    "        hist.append(f)\n",
    "#     hist = [x/sum(hist) if sum(hist) > 0 else 0 for x in hist]\n",
    "    duno_matriz.append(hist)\n",
    "from scipy.spatial import distance\n",
    "JSD_N = []\n",
    "for line in duno_matriz:\n",
    "    dist = distance.jensenshannon(HN, line)\n",
    "    JSD_N.append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(JSD_S)\n",
    "a = a.fillna(0)\n",
    "group_values_S = {\n",
    "    '<0.6': len(a[a[0]<0.6]),\n",
    "    '0.6 < x < 0.7': len(a[(a[0]>=0.6) & (a[0]<0.7)]),\n",
    "    '0.7 < x < 0.8': len(a[(a[0]>=0.7) & (a[0]<0.8)]),\n",
    "    '0.8 < x < 0.9': len(a[(a[0]>=0.8) & (a[0]<0.9)]),\n",
    "    '0.9 < x < 1': len(a[(a[0]>=0.9) & (a[0]<1)]),\n",
    "    '> 1': len(a[(a[0]>=1)])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.DataFrame(JSD_N)\n",
    "b = b.fillna(0)\n",
    "group_values_NS = {\n",
    "    '<0.6': len(b[b[0]<0.6]),\n",
    "    '0.6 < x < 0.7': len(b[(b[0]>=0.6) & (b[0]<0.7)]),\n",
    "    '0.7 < x < 0.8': len(b[(b[0]>=0.7) & (b[0]<0.8)]),\n",
    "    '0.8 < x < 0.9': len(b[(b[0]>=0.8) & (b[0]<0.9)]),\n",
    "    '0.9 < x < 1': len(b[(b[0]>=0.9) & (b[0]<1)]),\n",
    "    '> 1': len(b[(b[0]>=1)])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({ 'Comentários não sexistas': JSD_N, 'Comentários sexistas': JSD_S})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = data.hist(sharey=True,\n",
    "                 sharex=True,\n",
    "                 bins=25,\n",
    "                 grid=True,\n",
    "                 figsize=(20,8),\n",
    "                 color=char_colors[1],\n",
    "                 zorder=2,\n",
    "                 ylabelsize=14,\n",
    "                 xlabelsize=14,\n",
    "                 rwidth=0.9)\n",
    "plt.suptitle(\"Distância de Jensen Chanel dos comentário sem rótulo\", fontsize=14)\n",
    "plt.savefig('./images/jsd_distances.png',\n",
    "         orientation='landscape',\n",
    "         format='png',\n",
    "         bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    votes_per_user = pd.read_csv('data/votes-per-user.csv')\n",
    "except:\n",
    "    print('Retrieving from original database...')\n",
    "    votes_per_user = pd.read_sql_query('''\n",
    "    with q as (\n",
    "        select v.vote_id,\n",
    "           c.comment_id,\n",
    "           v.user_id,\n",
    "           v.gender,\n",
    "           date_part('year',age(v.birth)) as age,\n",
    "           c.content,\n",
    "           v.vote,\n",
    "           v.avg as label\n",
    "    from comments c\n",
    "             join (select v1.comment_id,\n",
    "                          r.avg,\n",
    "                          v1.vote_id,\n",
    "                          v1.serial_id as user_id,\n",
    "                          v1.user_id as id,\n",
    "                          v1.vote,\n",
    "                          v1.gender,\n",
    "                          v1.birth\n",
    "                   from (\n",
    "                        select u.serial_id,v2.vote,v2.vote_id, u.gender,v2.comment_id, u.birth, v2.user_id\n",
    "                        from votes v2 join ( select ROW_NUMBER() OVER(ORDER BY (SELECT 1)) AS serial_id,* from users) u on v2.user_id = u.username\n",
    "                       ) v1 join results r on v1.comment_id = r.comment_id) v on c.comment_id = v.comment_id\n",
    "    )\n",
    "    select * from q;\n",
    "    ''', conn)\n",
    "    votes_per_user.to_csv('data/votes-per-user.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_per_user.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = votes_per_user.groupby(by='comment_id').count()\n",
    "a.groupby(by='vote_id').count()['vote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_per_user.groupby(by='gender').count()['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_sql_query('select count(qtd), qtd from (select count(vote_id) as qtd from votes group by user_id) as u group by qtd;', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.loc[:, ~x.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.set_index('qtd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =x.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = x.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = x['count'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list(x['qtd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(list(x['qtd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(list(x['qtd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x['qtd'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = c\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = b \n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Programming language usage')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix to Manual Classifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def matriz_confusao(gender,df):\n",
    "  df = df[(votes_per_user['gender']==gender) & (votes_per_user['label']!=0.5)]\n",
    "  y_true = [float(x) for x in list(df['label'].apply(define_class))]\n",
    "  y_pred = [float(x) for x in list(df['vote'].apply(define_class))]\n",
    "  return pd.DataFrame(confusion_matrix(y_true, y_pred, labels=[1,0]), columns=['Sr','Nr'], index=['Sv','Nv'])\n",
    "\n",
    "female_confussion_matrix = matriz_confusao('fem',votes_per_user)\n",
    "male_confussion_matrix = matriz_confusao('masc',votes_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_confussion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_confussion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recal_f1(matrix):\n",
    "  tp = matrix['Sr']['Sv']\n",
    "  tp_fn = tp + matrix['Sr']['Nv']\n",
    "  tp_fp = tp + matrix['Nr']['Sv']\n",
    "  precision = tp/tp_fp\n",
    "  recall = tp/tp_fn\n",
    "  f1 = 2 * (precision * recall) / (precision + recall)\n",
    "  return precision, recall, f1\n",
    "\n",
    "precision_fem, recall_fem, f1_fem = precision_recal_f1(female_confussion_matrix)\n",
    "precision_male, recall_male, f1_male = precision_recal_f1(male_confussion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Female metrics: {precision_fem, recall_fem, f1_fem}')\n",
    "print(f'Male metrics: {precision_male, recall_male, f1_male}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating vote info by gender/age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_list = list(votes_per_user.age.unique())\n",
    "age_list.sort()\n",
    "\n",
    "qtd_votes_per_age = dict({})\n",
    "\n",
    "qtd_female_votes_per_age = dict({})\n",
    "qtd_sexist_female_votes_per_age = dict({})\n",
    "qtd_not_sexist_female_votes_per_age = dict({})\n",
    "\n",
    "qtd_correct_sexist_female_votes_per_age = dict({})\n",
    "qtd_correct_not_sexist_female_votes_per_age = dict({})\n",
    "qtd_incorrect_sexist_female_votes_per_age = dict({})\n",
    "qtd_incorrect_not_sexist_female_votes_per_age = dict({})\n",
    "\n",
    "qtd_male_votes_per_age = dict({})\n",
    "qtd_sexist_male_votes_per_age = dict({})\n",
    "qtd_not_sexist_male_votes_per_age = dict({})\n",
    "\n",
    "qtd_correct_sexist_male_votes_per_age = dict({})\n",
    "qtd_correct_not_sexist_male_votes_per_age = dict({})\n",
    "qtd_incorrect_sexist_male_votes_per_age = dict({})\n",
    "qtd_incorrect_not_sexist_male_votes_per_age = dict({})\n",
    "\n",
    "for age in age_list:\n",
    "  qtd_votes_per_age[age] = votes_per_user[votes_per_user['age']==age].shape[0]\n",
    "for age in age_list:\n",
    "  qtd_female_votes_per_age[age] = votes_per_user[(votes_per_user['age']==age) & (votes_per_user['gender']=='fem')].shape[0]\n",
    "for age in age_list:\n",
    "  qtd_male_votes_per_age[age] = votes_per_user[(votes_per_user['age']==age) & (votes_per_user['gender']=='masc')].shape[0]\n",
    "\n",
    "for age in age_list:\n",
    "  qtd_sexist_female_votes_per_age[age] = votes_per_user[(votes_per_user['age']==age) & (votes_per_user['gender']=='fem') & (votes_per_user['vote']==1)].shape[0]\n",
    "for age in age_list:\n",
    "  qtd_not_sexist_female_votes_per_age[age] = votes_per_user[(votes_per_user['age']==age) & (votes_per_user['gender']=='fem') & (votes_per_user['vote']==0)].shape[0]\n",
    "\n",
    "for age in age_list:\n",
    "  qtd_sexist_male_votes_per_age[age] = votes_per_user[(votes_per_user['age']==age) & (votes_per_user['gender']=='masc') & (votes_per_user['vote']==1)].shape[0]\n",
    "for age in age_list:\n",
    "  qtd_not_sexist_male_votes_per_age[age] = votes_per_user[(votes_per_user['age']==age) & (votes_per_user['gender']=='masc') & (votes_per_user['vote']==0)].shape[0]\n",
    "\n",
    "for age in age_list:\n",
    "  qtd_correct_sexist_female_votes_per_age[age] = votes_per_user[(votes_per_user['age']==age) & (votes_per_user['gender']=='fem') & (votes_per_user['vote']==1) & (votes_per_user['label']>0.5)].shape[0]\n",
    "for age in age_list:\n",
    "  qtd_correct_not_sexist_female_votes_per_age[age] = votes_per_user[(votes_per_user['age']==age) & (votes_per_user['gender']=='fem') & (votes_per_user['vote']==0) & (votes_per_user['label']<0.5)].shape[0]\n",
    "for age in age_list:\n",
    "  qtd_incorrect_sexist_female_votes_per_age[age] = votes_per_user[(votes_per_user['age']==age) & (votes_per_user['gender']=='fem') & (votes_per_user['vote']==0) & (votes_per_user['label']>0.5)].shape[0]\n",
    "for age in age_list:\n",
    "  qtd_incorrect_not_sexist_female_votes_per_age[age] = votes_per_user[(votes_per_user['age']==age) & (votes_per_user['gender']=='fem') & (votes_per_user['vote']==1) & (votes_per_user['label']<0.5)].shape[0]\n",
    "\n",
    "for age in age_list:\n",
    "  qtd_correct_sexist_male_votes_per_age[age] = votes_per_user[(votes_per_user['age']==age) & (votes_per_user['gender']=='masc') & (votes_per_user['vote']==1) & (votes_per_user['label']>0.5)].shape[0]\n",
    "for age in age_list:\n",
    "  qtd_correct_not_sexist_male_votes_per_age[age] = votes_per_user[(votes_per_user['age']==age) & (votes_per_user['gender']=='masc') & (votes_per_user['vote']==0) & (votes_per_user['label']<0.5)].shape[0]\n",
    "for age in age_list:\n",
    "  qtd_incorrect_sexist_male_votes_per_age[age] = votes_per_user[(votes_per_user['age']==age) & (votes_per_user['gender']=='masc') & (votes_per_user['vote']==0) & (votes_per_user['label']>0.5)].shape[0]\n",
    "for age in age_list:\n",
    "  qtd_incorrect_not_sexist_male_votes_per_age[age] = votes_per_user[(votes_per_user['age']==age) & (votes_per_user['gender']=='masc') & (votes_per_user['vote']==1) & (votes_per_user['label']<0.5)].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pizza(title, labels, values):\n",
    "  fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(aspect=\"equal\"))\n",
    "  recipe = labels\n",
    "  data = values\n",
    "  def func(pct, allvals):\n",
    "      absolute = int(pct/100.*np.sum(allvals))\n",
    "      return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n",
    "\n",
    "  wedges, texts, autotexts = ax.pie(data, autopct=lambda pct: func(pct, data),\n",
    "                                    textprops=dict(color=\"w\"), colors=char_colors)\n",
    "\n",
    "  lgd = ax.legend(wedges, recipe,\n",
    "            title=\"Classes\",\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "  plt.tight_layout()\n",
    "  plt.title(title,**csfont)\n",
    "  plt.setp(autotexts, size=15, weight=\"bold\")\n",
    "  plt.savefig(f'./images/{stringcase.snakecase(title)}.png',\n",
    "             orientation='landscape',\n",
    "             format='png',\n",
    "             bbox_extra_artists=(lgd,),\n",
    "             bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pizza('Comentários sexistas (Soma)',\n",
    "           ['Likes','Dislikes'],\n",
    "           [temp_comments.get('sexist').get('likes_qty'),\n",
    "            temp_comments.get('sexist').get('dislikes_qty')],\n",
    "          )\n",
    "plot_pizza('Comentários não sexistas (Soma)',\n",
    "           ['Likes','Dislikes'],\n",
    "           [temp_comments.get('not_sexist').get('likes_qty'),\n",
    "            temp_comments.get('not_sexist').get('dislikes_qty')]\n",
    "          )\n",
    "plot_pizza('Comentários não rotulados (Soma)',\n",
    "           ['Likes','Dislikes'],\n",
    "           [temp_comments.get('undefined').get('likes_qty'),\n",
    "            temp_comments.get('undefined').get('dislikes_qty')]\n",
    "          )\n",
    "plot_pizza('Comentários sexistas (Mediana)',\n",
    "           ['Likes','Dislikes'],\n",
    "           [temp_comments.get('sexist').get('likes_qty_median'),\n",
    "            temp_comments.get('sexist').get('dislikes_qty_median')]\n",
    "          )\n",
    "plot_pizza('Comentários não sexistas (Mediana)',\n",
    "           ['Likes','Dislikes'],\n",
    "           [temp_comments.get('not_sexist').get('likes_qty_median'),\n",
    "            temp_comments.get('not_sexist').get('dislikes_qty_median')]\n",
    "\n",
    "          )\n",
    "plot_pizza('Comentários não rotulados (Mediana)',\n",
    "           ['Likes','Dislikes'],\n",
    "           [temp_comments.get('undefined').get('likes_qty_median'),\n",
    "            temp_comments.get('undefined').get('dislikes_qty_median')]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doublebar_plot(df_1, df_2, ages, title):\n",
    "  X = np.array(ages)\n",
    "  Y1 = np.array(list((df_1).values()))\n",
    "  Y2 = np.array(list((df_2).values()))\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(20,8))\n",
    "\n",
    "  ax.axhline(0, color='black', lw=1)\n",
    "\n",
    "  ax.bar(X, +Y1, facecolor=char_colors[0], edgecolor='white', label='Homens', width=1)\n",
    "  ax.bar(X, -Y2, facecolor=char_colors[1], edgecolor='white', label='Mulheres', width=1)\n",
    "\n",
    "  lgd = ax.legend(fontsize = 13)\n",
    "\n",
    "  for x,y in zip(X,Y1):\n",
    "      ax.text(x, y, '%d%%' % y, ha='center', va= 'bottom', fontsize = 14)\n",
    "  for x,y in zip(X,Y2):\n",
    "      ax.text(x, -y-0.01, '%d%%' % y, ha='center', va= 'top', fontsize = 14)\n",
    "\n",
    "  plt.xlabel(\"Idades\")\n",
    "  plt.ylabel(\"Porcentagem de votos atribuídos\")\n",
    "  plt.xticks(X)\n",
    "  plt.savefig(f'./images/{stringcase.snakecase(title)}.png',\n",
    "             orientation='landscape',\n",
    "             format='png',\n",
    "             bbox_extra_artists=(lgd,),\n",
    "             bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corretos-sexista-genero-idade\n",
    "doublebar_plot(normalize_dict(qtd_correct_sexist_male_votes_per_age),\n",
    "               normalize_dict(qtd_correct_sexist_female_votes_per_age),\n",
    "               age_list,\n",
    "               title='corretos-sexista-genero-idade'\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorretos-sexista-genero-idade\n",
    "doublebar_plot(normalize_dict(qtd_incorrect_sexist_male_votes_per_age),\n",
    "               normalize_dict(qtd_incorrect_sexist_female_votes_per_age),\n",
    "               age_list,\n",
    "               title='incorretos-sexista-genero-idade'\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorretos-nao-sexista-genero-idade\n",
    "doublebar_plot(normalize_dict(qtd_incorrect_not_sexist_male_votes_per_age),\n",
    "               normalize_dict(qtd_incorrect_not_sexist_female_votes_per_age),\n",
    "               age_list,\n",
    "               title='incorretos-nao-sexista-genero-idade'\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corretos-nao-sexista-genero-idade\n",
    "doublebar_plot(normalize_dict(qtd_correct_not_sexist_male_votes_per_age),\n",
    "               normalize_dict(qtd_correct_not_sexist_female_votes_per_age),\n",
    "               age_list,\n",
    "               title='corretos-nao-sexista-genero-idade'\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação Automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_relevant_sexist_words = relevant_sexist_words\n",
    "\n",
    "sexist_vectorizer = TfidfVectorizer(\n",
    "    stop_words=[],\n",
    "    use_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=100,\n",
    ")\n",
    "not_sexist_vectorizer = TfidfVectorizer(\n",
    "    stop_words=[],\n",
    "    use_idf=False,\n",
    "    decode_error='replace',\n",
    "    max_features=100,\n",
    ")\n",
    "undefined_vectorizer = TfidfVectorizer(\n",
    "    stop_words=[],\n",
    "    use_idf=False,\n",
    "    decode_error='replace',\n",
    "    max_features=100,\n",
    ")\n",
    "\n",
    "sexist_doc = sexist_comments['content'].apply(lambda y: ' '.join([x for x in y.split() if x in tf_relevant_sexist_words]))\n",
    "not_sexist_doc = not_sexist_comments['content'].apply(lambda y: ' '.join([x for x in y.split() if x in tf_relevant_sexist_words]))\n",
    "undefined_doc = undefined_comments['content'].apply(lambda y: ' '.join([x for x in y.split() if x in tf_relevant_sexist_words]))\n",
    "\n",
    "sexist_tf = pd.DataFrame(sexist_vectorizer.fit_transform(sexist_doc).toarray())\n",
    "not_sexist_tf = pd.DataFrame(not_sexist_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
    "undefined_tf = pd.DataFrame(undefined_vectorizer.fit_transform(undefined_doc).toarray())\n",
    "\n",
    "likes_df = np.array(pd.concat([sexist_comments['likes'], not_sexist_comments['likes']]).fillna(0))\n",
    "dislikes_df = np.array(pd.concat([sexist_comments['dislikes'], not_sexist_comments['dislikes']]).fillna(0))\n",
    "char_qty_df = np.array(pd.concat([sexist_comments['char-qty'], not_sexist_comments['char-qty']]).fillna(0))\n",
    "word_qty_df = np.array(pd.concat([sexist_comments['word-qty'], not_sexist_comments['word-qty']]).fillna(0))\n",
    "\n",
    "sexist_y = sexist_comments['avg'].apply(lambda x: 1)\n",
    "not_sexist_y = not_sexist_comments['avg'].apply(lambda x: 0)\n",
    "undefined_y = undefined_comments['avg'].apply(lambda x: -1)\n",
    "\n",
    "y_df = np.array(pd.concat([sexist_y, not_sexist_y]))\n",
    "\n",
    "tf_dataframe = pd.concat([sexist_tf_unigrams, not_sexist_tf]).fillna(0)\n",
    "tf_dataframe.to_csv('./data/tf_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(dataframe, sub):\n",
    "    X_df = dataframe[dataframe.columns[:dataframe.shape[1] -sub -1]]\n",
    "    M = np.concatenate([X_df],axis=1)\n",
    "    X = pd.DataFrame(M)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(file_path, method, params, method_name, X_, y, reload=False):\n",
    "    tm = time.time()\n",
    "\n",
    "    if reload:\n",
    "        print('Executing Grid Search to %s.' % method_name)\n",
    "        model = GridSearchCV(method, param_grid=params, cv=cv, n_jobs=8)\n",
    "        model.fit(X_, y)\n",
    "        pickle.dump(model, open(file_path, 'wb'))\n",
    "    else:\n",
    "        try:\n",
    "            print('Reading %s. Model' % method_name)\n",
    "            model = pickle.load(open(file_path, 'rb'))\n",
    "        except:\n",
    "            print('Executing Grid Search to %s.' % method_name)\n",
    "            model = GridSearchCV(method, param_grid=params, cv=cv, n_jobs=8)\n",
    "            model.fit(X_, y)\n",
    "            pickle.dump(model, open(file_path, 'wb'))\n",
    "\n",
    "    print('Model loaded in: @ %d seconds' %\n",
    "                     ( time.time() - tm))\n",
    "    print(\"The best parameters are %s with a score of %0.2f\" % (model.best_params_, model.best_score_))\n",
    "    print('-------------------------------------------------------------------------------------------')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    print('...')\n",
    "    dataframe = pd.read_csv('./data/dataframe.csv')\n",
    "except:\n",
    "    print('---')\n",
    "    dataframe = tf_dataframe\n",
    "    dataframe['likes'] = likes_df\n",
    "    dataframe['dislikes'] = dislikes_df\n",
    "    dataframe['char-qty'] = char_qty_df\n",
    "    dataframe['word-qty'] = word_qty_df\n",
    "    dataframe['sexist'] = y_df\n",
    "    dataframe = dataframe.fillna(0)\n",
    "    dataframe = tf_dataframe.sample(frac=1)\n",
    "    dataframe.to_csv('.data/dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = dataframe['sexist']\n",
    "y = y_df.astype(int)\n",
    "X = select_features(dataframe, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features(dataframe, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf = select_features(dataframe, 4)\n",
    "X_tf_ld = select_features(dataframe, 2)\n",
    "X_tf_ld_wc = select_features(dataframe, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_params = dict(gamma=[0.1,1,10], C=[0.1, 1,10])\n",
    "svm_params = dict(gamma=np.logspace(-9, 3, 5), C=np.logspace(-2, 10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_tf = grid_search(\n",
    "    './data/svm-tf-grid-search-model',\n",
    "    SVC(),\n",
    "    svm_params,\n",
    "    'SVM with TF',\n",
    "    X_tf_unigrams, y,\n",
    "    # reload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_tf_ld = grid_search(\n",
    "    './data/svm-tf-ld-grid-search-model',\n",
    "    SVC(),\n",
    "    svm_params,\n",
    "    'SVM with TF, likes and dislikes',\n",
    "    X_tf_ld, y,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_tf_ld_wc = grid_search(\n",
    "    './data/svm-tf-ld-wc-grid-search-model',\n",
    "    SVC(),\n",
    "    svm_params,\n",
    "    'SVM with TF, likes, dislikes and words&chars quantity',\n",
    "    X_tf_ld_wc, y,            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = dict(n_neighbors=[3, 5, 11, 19], weights=['uniform', 'distance'], metric=['euclidean', 'manhattan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_tf = grid_search(\n",
    "    './data/knn-tf-grid-search-model',\n",
    "    KNeighborsClassifier(),\n",
    "    knn_params,\n",
    "    'KNN with TF',\n",
    "    X_tf_unigrams, y,\n",
    "    # reload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_tf_ld = grid_search(\n",
    "    './data/knn-tf-ld-grid-search-model',\n",
    "    KNeighborsClassifier(),\n",
    "    knn_params,\n",
    "    'KNN with TF, likes and dislikes',\n",
    "    X_tf_ld, y,\n",
    "    # reload=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_tf_ld_wc = grid_search(\n",
    "    './data/knn-tf-ld-wc-grid-search-model',\n",
    "    KNeighborsClassifier(),\n",
    "    knn_params,\n",
    "    'KNN with TF, likes, dislikes words and chars qauntitys',\n",
    "    X_tf_ld_wc, y,\n",
    "    reload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification reports to single test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Only TF\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_tf_unigrams, y, stratify=y)\n",
    "# TF, Likes and Dislikes\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X_tf_ld, y, stratify=y)\n",
    "# TF, Likes, Dislikes, Words and Chars quantities\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X_tf_ld_wc, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def report_results(X_test, y_test, model):\n",
    "    y_ = model.predict(X_test)\n",
    "    report = classification_report( y_test, y_ )\n",
    "    c_matrix = confusion_matrix(y_test, y_,  labels=[1,0])\n",
    "    return report, c_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_tf_report, svm_tf_cmatrix = report_results(X1_test, y1_test, svm_tf)\n",
    "svm_tf_ld_report, svm_tf_ld_cmatrix = report_results(X2_test, y2_test, svm_tf_ld)\n",
    "svm_tf_ld_wc_report, svm_tf_ld_wc_cmatrix = report_results(X3_test, y3_test, svm_tf_ld_wc)\n",
    "knn_tf_report, knn_tf_cmatrix = report_results(X1_test, y1_test, knn_tf)\n",
    "knn_tf_ld_report, knn_tf_ld_cmatrix = report_results(X2_test, y2_test, knn_tf_ld)\n",
    "knn_tf_ld_wc_report, knn_tf_ld_wc_cmatrix = report_results(X3_test, y3_test, knn_tf_ld_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('>>> SVM with TF results')\n",
    "print(svm_tf_report)\n",
    "print('Confusion matrix')\n",
    "print(pd.DataFrame(svm_tf_cmatrix, columns=['T','F'], index=['F','T']))\n",
    "print('>>> SVM with TF, Likes and Dislikes results')\n",
    "print(svm_tf_ld_report)\n",
    "print('Confusion matrix')\n",
    "print(pd.DataFrame(svm_tf_ld_cmatrix, columns=['T','F'], index=['F','T']))\n",
    "print('>>> SVM with TF, Likes, Dislikes, Words and Chars results')\n",
    "print(svm_tf_ld_wc_report)\n",
    "print('Confusion matrix')\n",
    "print(pd.DataFrame(svm_tf_ld_wc_cmatrix, columns=['T','F'], index=['F','T']))\n",
    "print('>>> KNN with TF results')\n",
    "print(knn_tf_report)\n",
    "print('Confusion matrix')\n",
    "print(pd.DataFrame(knn_tf_cmatrix, columns=['T','F'], index=['F','T']))\n",
    "print('>>> KNN with TF, Words e Chars quantities results')\n",
    "print(knn_tf_ld_report)\n",
    "print('Confusion matrix')\n",
    "print(pd.DataFrame(knn_tf_ld_cmatrix, columns=['T','F'], index=['F','T']))\n",
    "print('>>> KNN with TF, Likes, Dislikes, Words e Chars quantities results')\n",
    "print(knn_tf_ld_wc_report)\n",
    "print('Confusion matrix')\n",
    "print(pd.DataFrame(knn_tf_ld_wc_cmatrix, columns=['T','F'], index=['F','T']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not labeled classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = np.array(pd.concat([undefined_y]))\n",
    "tf_dataframe = pd.concat([undefined_tf]).fillna(0)\n",
    "dataframe = tf_dataframe\n",
    "dataframe['sexist'] = y_df\n",
    "dataframe = dataframe.fillna(0)\n",
    "dataframe = tf_dataframe.sample(frac=1)\n",
    "\n",
    "y_df = dataframe['sexist']\n",
    "y = y_df.astype(int)\n",
    "X = select_features(dataframe, 0)\n",
    "\n",
    "y_ = svm_tf.predict(X)\n",
    "unique, counts = np.unique(y_, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undefined_comments = pd.concat([undefined_comments, pd.DataFrame(y_)])\n",
    "undefined_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
